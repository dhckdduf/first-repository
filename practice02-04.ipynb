{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNnxy9B7wse5F7h1wQef5fe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhckdduf/first-repository/blob/main/practice02-04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5sXJWiI4wrZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "\n",
        "# 로이터 데이터셋 로드\n",
        "from tensorflow.keras.datasets import reuters\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# 데이터 로드 (num_words=10000은 가장 많이 등장하는 10,000개의 단어만 사용)\n",
        "max_words = 10000\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words)\n",
        "\n",
        "# 데이터를 원-핫 인코딩 벡터로 변환\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "\n",
        "# 라벨을 원-핫 벡터로 변환\n",
        "num_classes = np.max(y_train) + 1\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# 모델 생성 함수\n",
        "def build_model(reg=None, dropout_rate=None):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Dense(64, activation='relu', input_shape=(max_words,),\n",
        "                           kernel_regularizer=reg))\n",
        "    if dropout_rate:\n",
        "        model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.Dense(64, activation='relu', kernel_regularizer=reg))\n",
        "    if dropout_rate:\n",
        "        model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 각 모델 학습 및 비교\n",
        "epochs = 20\n",
        "batch_size = 512\n",
        "\n",
        "# (1) 기본 모델\n",
        "model_basic = build_model()\n",
        "history_basic = model_basic.fit(x_train, y_train,\n",
        "                                epochs=epochs, batch_size=batch_size,\n",
        "                                validation_data=(x_test, y_test),\n",
        "                                verbose=0)\n",
        "\n",
        "# (2) 가중치 규제 적용 모델 (L2 정규화)\n",
        "model_l2 = build_model(reg=regularizers.l2(0.001))\n",
        "history_l2 = model_l2.fit(x_train, y_train,\n",
        "                          epochs=epochs, batch_size=batch_size,\n",
        "                          validation_data=(x_test, y_test),\n",
        "                          verbose=0)\n",
        "\n",
        "# (3) 드롭아웃 적용 모델\n",
        "model_dropout = build_model(dropout_rate=0.5)\n",
        "history_dropout = model_dropout.fit(x_train, y_train,\n",
        "                                    epochs=epochs, batch_size=batch_size,\n",
        "                                    validation_data=(x_test, y_test),\n",
        "                                    verbose=0)\n",
        "\n",
        "# 학습 곡선 비교\n",
        "def plot_history(histories, title):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # 손실 곡선\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for label, history in histories.items():\n",
        "        plt.plot(history.history['loss'], label=f'{label} (train)')\n",
        "        plt.plot(history.history['val_loss'], linestyle='dashed', label=f'{label} (val)')\n",
        "    plt.title(f'{title} - Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # 정확도 곡선\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for label, history in histories.items():\n",
        "        plt.plot(history.history['accuracy'], label=f'{label} (train)')\n",
        "        plt.plot(history.history['val_accuracy'], linestyle='dashed', label=f'{label} (val)')\n",
        "    plt.title(f'{title} - Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# 학습 곡선 비교\n",
        "histories = {\n",
        "    'Basic': history_basic,\n",
        "    'L2 Regularization': history_l2,\n",
        "    'Dropout': history_dropout\n",
        "}\n",
        "\n",
        "plot_history(histories, 'Reuters Classification Model')\n"
      ]
    }
  ]
}